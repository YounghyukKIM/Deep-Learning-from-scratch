{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "591cea02",
   "metadata": {},
   "source": [
    "1. 신경망 학습의 목표\n",
    "    - 신경망 학습의 목적은 손실 함수(Loss Function) 를 가능한 한 낮추는 가중치/편향 매개변수를 찾는 것이다.\n",
    "        - 최적화(Optimization): 손실을 최소화하는 매개변수 찾기\n",
    "        - 기울기(Gradient)를 이용해서 최적화 수행\n",
    "        - 대표적 방법 : 확률적 경사 하강법 (SGD)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c1dfe",
   "metadata": {},
   "source": [
    "2. 확률적 경사 하강법 (SGD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1956fca6",
   "metadata": {},
   "source": [
    "- 2.1 수식\n",
    "    - $W \\leftarrow W - \\eta \\frac{\\partial L}{\\partial W}$\n",
    "        - $W$ : 가중치 매개변수\n",
    "        - $\\frac{\\partial L}{\\partial W}$ : 손실 함수에 대한 W의 기울기\n",
    "        - $\\eta $ : 학습률(learning rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaaec82",
   "metadata": {},
   "source": [
    "- 2.2 SGD 클래스 구현\n",
    "    - 학습률(lr)을 곱한 기울기를 매개변수에서 빼준다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c25f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8fe9fb",
   "metadata": {},
   "source": [
    "3. SGD의 단점\n",
    "    - 특정 방향(특히 y축 방향)으로 기울기가 큰 경우, 지그재그로 느리게 수렴하는 문제가 있다.\n",
    "        - 비등방성 함수(anisotropic function)에서는 비효율적\n",
    "        - 개선 방법 필요 → 모멘텀, AdaGrad, Adam 등장\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc60709c",
   "metadata": {},
   "source": [
    "4. 모멘텀 (Momentum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab1e80e",
   "metadata": {},
   "source": [
    "- 4.1 수식\n",
    "    - $v \\leftarrow \\alpha v - \\eta \\frac{\\partial L}{\\partial W}$\n",
    "    - $W←W+v$\n",
    "        - $v$ : 속도(velocity)\n",
    "        - $α$ : 모멘텀 계수 (보통 0.9)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebad705",
   "metadata": {},
   "source": [
    "- 4.2 Momentum 클래스 구현\n",
    "    - 이전 이동 방향을 고려해 빠르게 수렴하게 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed5147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {key: np.zeros_like(val) for key, val in params.items()}\n",
    "\n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "            params[key] += self.v[key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacd6ee2",
   "metadata": {},
   "source": [
    "5. AdaGrad\n",
    "    - 학습이 진행될수록 학습률을 자동으로 줄이는 방법.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fca2935",
   "metadata": {},
   "source": [
    "- 5.1 수식\n",
    "    - $h \\leftarrow h + \\left( \\frac{\\partial L}{\\partial W} \\right)^2$\n",
    "    - $W \\leftarrow W - \\eta \\frac{1}{\\sqrt{h}} \\frac{\\partial L}{\\partial W}$\n",
    "        - $h$ : 과거 기울기의 제곱합\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39423a05",
   "metadata": {},
   "source": [
    "- 5.2 AdaGrad 클래스 구현\n",
    "    - 학습을 진행할수록 학습률이 점점 줄어든다.\n",
    "    - 오버플로 방지를 위해 분모에 아주 작은 수 1e−7 추가."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212053fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {key: np.zeros_like(val) for key, val in params.items()}\n",
    "\n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f5dff",
   "metadata": {},
   "source": [
    "6. Adam\n",
    "    - Momentum + AdaGrad를 합친 최적화 기법. 현재 가장 널리 쓰이는 방법 중 하나.\n",
    "        - RMSProp + 모멘텀 + 편향 보정(Bias Correction) 사용 (지금 단계에서는 직접 구현 생략)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2a6ccf",
   "metadata": {},
   "source": [
    "7. 최적화 기법 비교 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8207de0",
   "metadata": {},
   "source": [
    "| 방법 | 특징 | 장단점 |\n",
    "|:----|:-----|:-----|\n",
    "| SGD | 가장 기본적인 경사 하강법 | 느릴 수 있음 |\n",
    "| Momentum | 운동량을 고려하여 빠르게 수렴 | 튜닝 필요(모멘텀 계수) |\n",
    "| AdaGrad | 학습률 자동 감소 | 지나치게 학습률이 작아질 수 있음 |\n",
    "| Adam | Momentum + AdaGrad 융합 | 현재 가장 널리 사용됨 |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
